{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<center><h1> AI Fairness 360 </h1></center>\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "Welcome to the handson!!!\n",
    "\n",
    "- In this handson you will be **detecting and mitigating gender bias on adult dataset using AI Fairness 360 (aif360)**\n",
    "- You will be implementing **BinaryLabelDatasetMetric** to detect bias and **DisparateImpactRemover algorithm** to mitigate bias in the dataset\n",
    "- Follow the instruction provided for cell to write the code in each cell.\n",
    "- Run the below cell for to import necessary packages to read\n",
    "- Before submit your notebook. Restart the kernel and run all the cell. Make sure that any cell shouldn't cause any error or problem.\n",
    "- Don't forget to run the last cell in the jupyter notebook, failing which your efforts will be invalid.\n",
    "- Don't delete any cell given in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this scenario, will utilize AI Fairness 360 (aif360) to detect and mitigate bias. We will use the AdultDataset, splitting it into a training and test dataset. We will look for bias in the creation of a machine learning model to predict if user has income-per-year > 50k. The protected attribute will be \"gender\", with \"1\" (Male) and \"0\" (Female) being the values for the privileged and unprivileged groups, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from test_aifairness import bias_mitigation\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 1: Load AdultDataset dataset and set the protected attribute to be gender. Keep the `age`, `education-num`, `capital-gain`, `capital-loss`, `hours-per-week` features. Change protected attribute \"gender\" will be, with \"1\" (Male) and \"0\" (Female) being the values for the privileged and unprivileged groups.**\n",
    "\n",
    "- Assign dataset into the variable `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_map=[{1: '>50K', 0: '<=50K'}],\n",
    "protected_attribute_maps= [{1: 'Male', 0: 'Female'}]\n",
    "data = AdultDataset(protected_attribute_names=['gender'],privileged_classes=[['Male']],categorical_features=[],features_to_keep=[ 'age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'])#, metadata={'label_map': label_map,'protected_attribute_maps': protected_attribute_maps})\n",
    "#print(data.convert_to_dataframe()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 2: Split the original dataset into training and testing datasets *`(Use attributes: num_or_size_splits = [0.8], shuffle = True and seed = 35`* and Set two variables `privileged_groups` and `unprivileged_groups` for the privileged (1) and unprivileged (0) values for the gender attribute. (Example: `group = [{attribute: value}]`)**\n",
    "\n",
    "- Assign train dataset into the variable `data_train`\n",
    "- Assign test dataset into the variable `data_test`\n",
    "- Assign privileged_groups into the variable `privileged_groups`\n",
    "- Assign unprivileged_groups into the variable `unprivileged_groups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = data_train, data_test = data.split([0.8], shuffle=True,seed = 35)\n",
    "'''from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(copy=False)\n",
    "data_train.features = scaler.fit_transform(data_train.features)\n",
    "data_test.features = scaler.fit_transform(data_test.features)'''\n",
    "privileged_groups = [{'gender': 1}]\n",
    "unprivileged_groups = [{'gender': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Compute fairness metric on original training dataset\n",
    "--------------------------------\n",
    "\n",
    "Will use the training dataset and testing dataset in this handson, a normal workflow would also use a test dataset for assessing the efficacy (accuracy, fairness, etc.) during the development of a machine learning model.\n",
    "\n",
    "we've identified the protected attribute 'gender' and defined privileged and unprivileged values, we can use aif360 to detect bias in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 3: Compute fairness metric by calculating disparate_impact for privileged and unprivileged groups using original train dataset (Hint: Use BinaryLabelDatasetMetric).**\n",
    "\n",
    "- Assign disparate impact value into the variable `disparate_impact_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_train = BinaryLabelDatasetMetric(data_train, unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\n",
    "disparate_impact_val = metric_train.disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.361211\n",
      "\n",
      "\n",
      "The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80% of their proportion of the privilege group, this is a disparate impact violation. However, you may decide to increase this for your business. In this scenario, we are below the threshold of 0.8 so we deem this to be unfair.\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % disparate_impact_val)\n",
    "\n",
    "if disparate_impact_val < 0.8:\n",
    "    print(\"\\n\\nThe industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80% of their proportion of the privilege group, this is a disparate impact violation. However, you may decide to increase this for your business. In this scenario, we are below the threshold of 0.8 so we deem this to be unfair.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Run the below cell, to split `X_train`, `X_test`, `y_train`, `y_test`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "index = data_train.feature_names.index('gender')\n",
    "\n",
    "X_train = np.delete(data_train.features, index, axis=1)\n",
    "X_test = np.delete(data_test.features, index, axis=1)\n",
    "y_train = data_train.labels.ravel()\n",
    "y_test = data_test.labels.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 4: Train the machine learning model and predict the income-per-year > 50k. (Hint: Use LogisticRegression with `class_weight='balanced', solver='liblinear'`).**\n",
    "\n",
    "- Assign predicted value into the variable `predict_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear',class_weight='balanced')\n",
    "model = lr.fit(X_train, y_train)\n",
    "predict_val = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Let's Compute fairness metric by calculating disparate_impact for privileged and unprivileged groups using predicted values (Hint: Use BinaryLabelDatasetMetric).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.649110\n",
      "\n",
      "\n",
      "As with our initial data, we see a substantially greater proportion of the privileged group recieving the favourable output and identify this to be unfair.\n"
     ]
    }
   ],
   "source": [
    "test_pred = data_test.copy()\n",
    "test_pred.labels = predict_val\n",
    "\n",
    "metric_predict = BinaryLabelDatasetMetric(test_pred, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "disparate_impact_val2 = metric_predict.disparate_impact()\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % disparate_impact_val2)\n",
    "\n",
    "if disparate_impact_val2 < 0.8:\n",
    "    print(\"\\n\\nAs with our initial data, we see a substantially greater proportion of the privileged group recieving the favourable output and identify this to be unfair.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "###  Mitigate bias by transforming the original dataset\n",
    "-------------------------------\n",
    "The previous step showed that the privileged group was getting more positive outcomes in the predict values. Since this is not desirable, we are going to try to mitigate this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 5: Mitigate bias by transforming the original training and testing dataset (Hint: Use DisparateImpactRemover algorithm).**\n",
    "\n",
    "- Assign transformed training dataset into the variable `data_transf_train`\n",
    "- Assign transformed testing dataset into the variable `data_transf_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = DisparateImpactRemover(repair_level=1.0)\n",
    "data_transf_train = di.fit_transform(data_train)\n",
    "data_transf_test = di.fit_transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Run the below cell, to split `X_train`, `X_test`, `y_train`, `y_test`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "index = data_transf_train.feature_names.index('gender')\n",
    "\n",
    "X_train = np.delete(data_transf_train.features, index, axis=1)\n",
    "X_test = np.delete(data_transf_test.features, index, axis=1)\n",
    "y_train = data_transf_train.labels.ravel()\n",
    "y_test = data_transf_test.labels.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Compute fairness metric on transformed dataset\n",
    "--------------------------------\n",
    "\n",
    "Now that we have a transformed dataset, we can check how effective it was in removing bias by using the same metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 6: Now Train the machine learning model using transformed dataset (`X_train`, `X_test`, `y_train`, `y_test`) (Hint: Use LogisticRegression with `class_weight='balanced', solver='liblinear'`) and Compute fairness metric by calculating mean difference for privileged and unprivileged groups using predicted values (Hint: Use BinaryLabelDatasetMetric).**\n",
    "\n",
    "- Assign disparate impact value into the variable `disparate_impact_val_di`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please refer above cells for calculating fairness value for predicted values\n",
    "lr = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "model = lr.fit(X_train, y_train)\n",
    "predict_val = model.predict(X_test)\n",
    "test_pred = data_test.copy()\n",
    "test_pred.labels = predict_val\n",
    "metric_predict = BinaryLabelDatasetMetric(test_pred, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "disparate_impact_val_di = metric_predict.disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.900934\n",
      "\n",
      "\n",
      "Now it is fair!\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % disparate_impact_val_di)\n",
    "\n",
    "if disparate_impact_val_di > 0.8:\n",
    "    print(\"\\n\\nNow it is fair!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Run the below cells to save your answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "bias_mitigation.save_ans1(data)\n",
    "bias_mitigation.save_ans2(data_train, data_test, privileged_groups, unprivileged_groups)\n",
    "bias_mitigation.save_ans3(disparate_impact_val)\n",
    "bias_mitigation.save_ans4(predict_val)\n",
    "bias_mitigation.save_ans5(data_transf_train, data_transf_test)\n",
    "bias_mitigation.save_ans6(disparate_impact_val_di)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
